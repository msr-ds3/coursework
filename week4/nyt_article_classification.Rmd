---
title: "NYTimes article classification"
author: "Your Name"
date: '`r Sys.time()`'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
library(here)
library(tm)
library(Matrix)
library(glmnet)
library(ROCR)
library(tidyverse)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

# Part A

Read business and world articles into a single data frame.

```{r read-articles}

```

Create a Corpus from the article snippets using the `VectorSource()` and `Corpus()` functions from the `tm` package. Then create a DocumentTermMatrix from the snippet Corpus, removing punctuation and numbers and convert the DocumentTermMatrix to a sparseMatrix, required by cv.glmnet using the provided funciton. See the `DocumentTermMatrix()` documentation for more details, which has parameters for all of the punctuation and number parsing.

```{r create-sparse-dtm}

# helper function
dtm_to_sparse <- function(dtm) {
 sparseMatrix(i=dtm$i, j=dtm$j, x=dtm$v, dims=c(dtm$nrow, dtm$ncol), dimnames=dtm$dimnames)
}

```


# Part B

Create a train / test split

```{r create-train-test}

```

Cross-validate on the training set using logistic regression with cv.glmnet, measuring auc. See documentation on `cv.glmnet()`, specifically the `family` and `type.measure` parameters.

```{r cross-validate}

```

Evaluate performance for the best-fit (`lambda.min`) model by plotting the ROC curve and printing the accuracy and AUC.

```{r evaluate-best-model}

```

# Part C

Count how many words have non-zero coefficients. Use the `coef()` and `tidy()` functions for `lambda.min`.

```{r count-nonzero-weights}

```

Print the words with the top 10 heighest weights for the Business section. Do the same for the World section. Use the `coef()` and `tidy()` functions for `lambda.min`.


```{r show-top-words}

```

Think about how this model would perform if you used it to classify data from today's newspaper compared to the data in the test set.

