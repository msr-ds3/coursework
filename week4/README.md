


# Day 1
  * See the [slides](tree-boost-forest.pdf) from Rob's lecture on Decision trees, boosting, and random forests
  * Also see these interactive tutorials on [decision trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) and [bias and variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)
  * Go through Lab 8.3.1 from [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/index.html)
  * Then do the exercise at the bottom of this [notebook](https://rpubs.com/dvorakt/248300) on predicting who survived on the Titanic
    * The notebook uses the C50 library, which may be difficult to install, so feel free to use `tree` instead <!-- or rpart -->
  * For reference, [this notebook](https://rpubs.com/ryankelly/dtrees) has more on regression and classification trees

<!--    * Try [rpart.plot](https://stackoverflow.com/a/48881163/76259) as an alternative to the native `plot()` function for trees -->

<!--
  * See [intro.py](intro.py) for in-class Python examples
  * Do the [Codecademy Python tutorial](https://www.codecademy.com/learn/python)
  * See [Learnpython's advanced tutorials](http://www.learnpython.org) on generators and list comprehensions


  * We had a guest lecture from [Hal Daume]() on natural language processing
    * Slides on [word sense disambiguation](http://www.cs.umd.edu/class/fall2016/cmsc723/slides/slides_05.pdf), [expectation maximization](http://www.cs.umd.edu/class/fall2016/cmsc723/slides/slides_06.pdf), and [word alignment](http://www.cs.umd.edu/class/fall2016/cmsc723/slides/slides_18.pdf)
    * The [Yarowsky algorithm](https://en.wikipedia.org/wiki/Yarowsky_algorithm) for word sense disambiguation 
    * [A statistical approach to machine translation](http://dl.acm.org/citation.cfm?id=92860)
    * See these interactive demos on [k-means](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) and [mixture models](http://davpinto.com/ml-simulations/#gaussian-mixture-density)
-->

